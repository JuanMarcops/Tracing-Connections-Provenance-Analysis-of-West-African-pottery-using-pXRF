---
title: "Tracing connections Analysis for paper"
author: "Juan-Marco Puerta Schardt"
date: "2025-03-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## loading packages and premade functions

```{r 1}
library(here)         # Loads the 'here' package to manage file paths more efficiently  
library(tidyverse)    # Loads the 'tidyverse' package for data cleaning and transformation  
library(factoextra)   # Provides visualization tools for multivariate statistical analysis  
library(FactoMineR)   # Supports multivariate statistical methods such as PCA and MCA  
library(MASS)         # Contains functions for discriminant analysis and statistical modeling
library(caret)        # Offers tools for predictive modeling and machine learning  
library(plotly)       # Enables interactive visualizations  

# Load custom functions from an external script  
source(here("R/RFA-Functions.R"))  

```

## loading registry and data

```{r 2}
protocol <- read.csv2(here("data/Messprotokoll_JAS.csv")) # Loading the Protocol of Measurements taken (including measured object, measurement per sherd, Region, Site)
```



```{r 3}
folder_path <- here("data/Messungsdaten")  
# Defines the file path to the "Messungsdaten" (measurement data) folder using the 'here' package  

measurements <- load_xrf(folder_path)  
# Loads all RFA measurement data using the custom function 'load_xrf'

```


```{r 4}
measurements <- measurements |>  
    mutate(Reading.No = as.numeric(Reading.No))  
# Converts the 'Reading.No' column in the 'measurements' dataset to numeric format  

RFA <- protocol |>  
  left_join(measurements, by = c("Nr" = "Reading.No")) |>  # Merges 'protocol' with 'measurements' based on 'Nr' and 'Reading.No'  
  filter(!is.na(User.Login)) |>  # Filters out rows where 'User.Login' is missing  
  dplyr::select_if(~ !all(is.na(.)))  # Removes columns that contain only NA values  

protocol |>  
  left_join(measurements, by = c("Nr" = "Reading.No")) |>  # Joins 'protocol' with 'measurements' again  
  filter(is.na(User.Login))  # Filters for rows where 'User.Login' is missing (possibly for debugging or checking missing matches)  

protocol <- protocol |>  
  filter(Nr %in% RFA$Nr)  # Keeps only the rows in 'protocol' where 'Nr' exists in the 'RFA' dataset  


```
## get values and errors

```{r }
# Process the RFA data to extract values, replacing "<LOD" with 0 and converting to numeric
RFA.val <- RFA |> 
  dplyr::select(Nr, Mo:Mg) |>  # Select columns from 'Nr' to 'Mg'
  dplyr::select(-contains(".Error")) |>  # Exclude columns containing ".Error"
  column_to_rownames("Nr") |>  # Set 'Nr' as row names
  mutate(across(everything(), ~ ifelse(grepl("<LOD", .), 0, as.character(.)))) |>  # Replace "<LOD" with 0
  mutate_all(funs(str_replace_all(., ",", "\\."))) |>  # Replace commas with periods
  mutate_all(as.numeric)  # Convert all columns to numeric

# Process the RFA data to extract error values
RFA.error <- RFA |> 
  dplyr::select(Nr, contains(".Error")) |>  # Select columns containing ".Error"
  column_to_rownames("Nr") |>   # Set 'Nr' as row names
    mutate_all(as.numeric)
```

```{r}
LOD_proportions <- RFA |> 
  # dplyr::select(where(~ any(grepl("<LOD", .))))  |> # Keep only columns containing "<LOD"
   summarise(across(everything(), ~ sum(grepl("<LOD", .)) / n())) |>  # Calculate proportion of "<LOD" per column
  pivot_longer(everything(), names_to = "Element", values_to = "Proportion_LOD") |>  # Convert to long format
  filter(Proportion_LOD > 0) |>  # Keep only columns with at least one "<LOD"
  arrange(desc(Proportion_LOD))  # Sort by proportion in descending order

bad.lod.elements <- LOD_proportions |> 
  filter(Proportion_LOD > 0.25) |> 
  pull(Element)

# Plot the data using ggplot2
ggplot(LOD_proportions, aes(x = reorder(Element, -Proportion_LOD), y = Proportion_LOD)) +
  geom_bar(stat = "identity", fill = "steelblue") +  # Bar plot
  labs(
    x = "Element",  # x-axis label
    y = "Anteil der <LOD-Werte",  # y-axis label
    title = "Anteil der <LOD-Werte pro Element"  # Plot title
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

##error

```{r}

RFA.error_prop <- RFA.error/RFA.val*100 

# Replace infinite values (Inf) with NA (e.g., where RFA.val was 0)
RFA.error_prop[RFA.error_prop == Inf] <- NA

# Summarize the relative error statistics (mean, sd, median) for each element
error_summary <- RFA.error_prop %>%
  summarise(across(everything(), list(
    mean = ~ mean(.x, na.rm = TRUE),  # Calculate mean
    sd = ~ sd(.x, na.rm = TRUE),      # Calculate standard deviation
    median = ~ median(.x, na.rm = TRUE)  # Calculate median
  ))) %>%
  pivot_longer(
    cols = everything(),  # Convert all columns to long format
    names_to = c("Element", "variable"),  # Split column names into "Element" and "variable"
    names_pattern = "(.*)_(.*)"  # Use regex to separate element names from statistics
  )

# Filter and print elements with mean relative error < 20%, sorted by value
error_summary |> 
  filter(variable == "mean") |>  # Filter for mean values
  arrange(value) |>  # Sort by mean value
  rename(mean = value) |> #Renaming value to mean
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name (remove suffixes)
  print()

# Filter and print elements with standard deviation of relative error, sorted by value
error_summary |> 
  filter(variable == "sd") |>  # Filter for standard deviation values
  arrange(value) |>  # Sort by standard deviation
    rename(sd = value) |> #Renaming value to sd
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name
  print()

# Filter and print elements with median relative error, sorted by value
error_summary |> 
  filter(variable == "median") |>  # Filter for median values
  arrange(value) |>  # Sort by median
      rename(sd = value) |> #Renaming value to median
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name
  print()

# Extract elements with mean relative error < 10% (excluding "Bal")
low.error.elements_10 <- error_summary |> 
  filter(variable == "mean" & value < 10) |>  # Filter for mean values below 10%
  arrange(value) |>  # Sort by mean value
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name
  filter(Element != "Bal") |>  # Exclude "Bal" (if present)
  pull(Element)  # Extract element names as a vector

# Extract elements with mean relative error < 15% (excluding "Bal")
low.error.elements_15 <- error_summary |> 
  filter(variable == "mean" & value < 15) |>  # Filter for mean values below 20%
  arrange(value) |>  # Sort by mean value
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name
  filter(Element != "Bal") |>  # Exclude "Bal" (if present)
  pull(Element)  # Extract element names as a vector

# Extract elements with mean relative error < 20% (excluding "Bal")
low.error.elements_20 <- error_summary |> 
  filter(variable == "mean" & value < 20) |>  # Filter for mean values below 20%
  arrange(value) |>  # Sort by mean value
  mutate(Element = str_extract(Element, "^[A-Za-z]+")) |>  # Extract element name
  filter(Element != "Bal") |>  # Exclude "Bal" (if present)
  pull(Element)  # Extract element names as a vector

all_elements <- RFA.val |> 
  dplyr::select(-Bal) |> #exclude Bal
  colnames() #extract names of elements

# Print the results
print(low.error.elements_10)
print(low.error.elements_20)
```

```{r}
# Ensure RFA.error_prop is in a tidy format (long format) for ggplot
RFA.error_prop_long <- RFA.error_prop %>%
  pivot_longer(cols = everything(), names_to = "Element", values_to = "Error_Proportion") %>%
  filter(!is.na(Error_Proportion))  # Remove NA values

# Create the boxplot
ggplot(RFA.error_prop_long, aes(x = Element, y = Error_Proportion)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +  # Boxplot with custom fill color
  labs(
    x = "Element",  # x-axis label
    y = "Relative Error (%)",  # y-axis label
    title = "Distribution of Relative Errors by Element"  # Plot title
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```


# LOD taken into account
```{r}
# Process the RFA data to extract values, replacing "<LOD" with the midpoint between the column mean and 0
RFA.val <- RFA |> 
  dplyr::select(Nr, Mo:Mg) |>  # Select columns from 'Nr' to 'Mg'
  dplyr::select(-contains(".Error")) |>  # Exclude columns containing ".Error"
  column_to_rownames("Nr") |>  # Set 'Nr' as row names
  mutate(across(everything(), ~ ifelse(grepl("<LOD", .), NA, as.character(.)))) |>  # Replace "<LOD" with NA
  mutate_all(funs(str_replace_all(., ",", "\\."))) |>  # Replace commas with periods
  mutate_all(as.numeric) |>  # Convert all columns to numeric
  mutate(across(everything(), ~ ifelse(is.na(.), min(., na.rm = TRUE) / 2, .))) |>  # Replace NA with midpoint between column mean and 0
 dplyr::select(where(~!any(is.infinite(.)))) #remove columns containing only na


```

## calibration
```{r}
# Load calibration data
calib_mean <- read.csv2(here("data/Calibration data/calib_mean.csv")) |> 
  dplyr::select(-Position)  # Remove the "Position" column

calib_aim <- read.csv2(here("data/Calibration data/calib_aim.csv")) |> 
  dplyr::select(-Probe)  # Remove the "Probe" column

# Keep only common columns between calib_mean and calib_aim
calib_mean <- calib_mean |>  
  dplyr::select(all_of(intersect(names(calib_aim), names(calib_mean)))) 

calib_aim <- calib_aim |>  
  dplyr::select(all_of(intersect(names(calib_aim), names(calib_mean))))
```

```{r}
# Initialize an empty list to store the linear regression models
models <- list()

# Perform linear regression for each column in calib_aim
for (col in names(calib_aim)) {
  # Combine calibration and target data into a dataframe
  data <- data.frame(y = calib_mean[[col]], x = calib_aim[[col]])
  
  # Create a linear model: y ~ x (y is the dependent variable, x is the independent variable)
  model <- lm(y ~ x, data = data)
  
  # Store the model in the list with the column name as the key
  models[[col]] <- model
  
  # Calculate the correlation and p-value between y and x
  correlation <- cor.test(data$y, data$x)
  
  # Print the correlation coefficient and p-value for the current column
  cat("Correlation and p-value for column", col, ":", correlation$estimate, "&", correlation$p.value, "\n")
}

# Display the list of models
print(models)

# Function to calibrate new data using the linear models
calibrate_data <- function(new_data, models) {
  # Create a copy of the new data to store the calibrated values
  calibrated_data <- new_data
  
  # Loop through each column in the new data
  for (col in names(new_data)) {
    # Check if a model exists for the current column
    if (col %in% names(models)) {
      # Extract the model for the current column
      model <- models[[col]]
      
      # Get the intercept and slope from the model coefficients
      intercept <- coef(model)[1]
      slope <- coef(model)[2]
      
      # Apply the calibration equation: y = slope * x + intercept
      calibrated_data[[col]] <- slope * new_data[[col]] + intercept
    }
  }
  
  # Return the calibrated data
  return(calibrated_data)
}

# Apply the calibration models to the new dataframe (calib_mean)
calibrated_data <- calibrate_data(calib_mean, models)

# Display the calibrated dataframe
print(calibrated_data)

# Apply the calibration models to RFA.val
RFA.val_cal <- calibrate_data(RFA.val, models)
```


## removing outliers


```{r}
remove_outliers_and_calculate_stat <- function(values, threshold = 1.5, method = "iqr", calc_sd = FALSE) {
  # Remove NA values from the input
  values <- na.omit(values)
  n <- length(values)

  # If fewer than 3 values exist or all values are the same, return mean or SD directly
  if (n < 3 || length(unique(values)) == 1) {  # NEW: Check if all values are identical
    if (calc_sd) {
      return(0)  # If all values are identical, the standard deviation is always 0
    } else {
      return(mean(values, na.rm = TRUE))
    }
  }

  # Check if the selected method is valid
  if (!method %in% c("z", "iqr", "modified_z", "dixon")) {
    stop("Invalid method. Use 'z' for Z-score, 'iqr' for Interquartile Range, 'modified_z' for Modified Z-Score, or 'dixon' for Dixon's Q-Test.")
  }

  # Z-score method
  if (method == "z") {
    mean_value <- mean(values, na.rm = TRUE)
    sd_value <- sd(values, na.rm = TRUE)

    # Ensure sd_value is not 0 to avoid division by zero
    if (sd_value == 0) {
      non_outliers <- values
    } else {
      z_scores <- (values - mean_value) / sd_value
      non_outliers <- values[abs(z_scores) <= threshold]
    }

  # Interquartile Range (IQR) method  
  } else if (method == "iqr") {  
    Q1 <- quantile(values, 0.25, na.rm = TRUE)
    Q3 <- quantile(values, 0.75, na.rm = TRUE)
    IQR_value <- Q3 - Q1
    lower_bound <- Q1 - threshold * IQR_value
    upper_bound <- Q3 + threshold * IQR_value
    non_outliers <- values[values >= lower_bound & values <= upper_bound]

  # Modified Z-score method  
  } else if (method == "modified_z") {  
    median_value <- median(values, na.rm = TRUE)
    mad_value <- mad(values, na.rm = TRUE)

    # Ensure mad_value is not 0 to avoid division by zero
    if (mad_value == 0) {
      non_outliers <- values
    } else {
      modified_z_scores <- 0.6745 * (values - median_value) / mad_value
      non_outliers <- values[abs(modified_z_scores) <= threshold]
    }

  # Dixon’s Q-test for outliers  
  } else if (method == "dixon") {  
    sorted_values <- sort(values)

    # If all values are identical, Dixon’s test is meaningless
    if (sorted_values[n] == sorted_values[1]) {
      non_outliers <- sorted_values
    } else {
      Q_min <- (sorted_values[2] - sorted_values[1]) / (sorted_values[n] - sorted_values[1])
      Q_max <- (sorted_values[n] - sorted_values[n - 1]) / (sorted_values[n] - sorted_values[1])

      critical_Q <- 0.941  # 95% confidence level

      # Check for NA values in Q-values (in case of division by zero)
      if (is.na(Q_min) | is.na(Q_max)) {
        non_outliers <- sorted_values
      } else {
        # Remove both outliers if both exceed the critical threshold
        if (Q_min > critical_Q & Q_max > critical_Q) {
          non_outliers <- sorted_values[2:(n - 1)]
        } else if (Q_min > critical_Q) {
          non_outliers <- sorted_values[-1]  # Remove only the smallest value
        } else if (Q_max > critical_Q) {
          non_outliers <- sorted_values[-n]  # Remove only the largest value
        } else {
          non_outliers <- sorted_values  # No outliers detected
        }
      }
    }
  }

  # Calculate either the mean or standard deviation percentage
  if (calc_sd) {
    mean_non_outliers <- mean(non_outliers, na.rm = TRUE)
    sd_non_outliers <- sd(non_outliers, na.rm = TRUE)

    # Ensure no division by


```


```{r}
# Not removing outliers bevor calculating mean
RFA.val_mean <- cbind(protocol, RFA.val_cal) |>  
  filter(Object != "Standart-TS") |>  # Remove rows where 'Object' is "Standart-TS"  
  group_by(Object) |>  # Group data by 'Object'  
  dplyr::select(-Nr) |>  # Remove 'Nr' column  
  summarise(
    across(where(is.numeric), remove_outliers_and_calculate_stat),  # Apply outlier removal function to numeric columns  
    across(where(is.character), first)  # Keep the first value for character columns  
  )

# Apply Dixon’s Q-test method for outlier removal and calculating mean
RFA.val_dix <- protocol |>  
  cbind(RFA.val_cal) |>  
  filter(Object != "Standart-TS") |>  
  group_by(Object) |>  
  dplyr::select(-Nr) |>  
  summarise(
    across(where(is.numeric), ~remove_outliers_and_calculate_stat(.x, method = "dixon")),  
    across(where(is.character), first)  
  )

# Apply Modified Z-score method for outlier removal and calculating mean
RFA.val_mz <- protocol |>  
  cbind(RFA.val_cal) |>  
  filter(Object != "Standart-TS") |>  
  group_by(Object) |>  
  dplyr::select(-Nr) |>  
  summarise(
    across(where(is.numeric), ~remove_outliers_and_calculate_stat(.x, method = "modified_z")),  
    across(where(is.character), first)  
  )

# Apply Z-score method for outlier removal with threshold 0.5 and calculating mean
RFA.val_z <- protocol |>  
  cbind(RFA.val_cal) |>  
  filter(Object != "Standart-TS") |>  
  group_by(Object) |>  
  dplyr::select(-Nr) |>  
  summarise(
    across(where(is.numeric), ~remove_outliers_and_calculate_stat(.x, method = "z", threshold = 0.5)),  
    across(where(is.character), first)  
  )

# Compute the mean for numeric columns and keep the first value for 

```


###comparing outlier by method
```{r}
detect_outliers <- function(values, method = "iqr") {
  values <- na.omit(values)  # Remove missing values
  n <- length(values)

  # If there are fewer than 3 values, outlier detection is not possible
  if (n < 3) {
    return(data.frame(value = values, outlier = FALSE))  
  }

  is_outlier <- rep(FALSE, n)  # Default: no outliers

  # Z-score method: Detects outliers based on standard deviations from the mean
  if (method == "z") {
    mean_value <- mean(values, na.rm = TRUE)
    sd_value <- sd(values, na.rm = TRUE)
    z_scores <- (values - mean_value) / sd_value
    is_outlier <- abs(z_scores) > 1.5  # Threshold for Z-score outlier detection

  # IQR method: Uses interquartile range to detect outliers
  } else if (method == "iqr") {
    Q1 <- quantile(values, 0.25, na.rm = TRUE)
    Q3 <- quantile(values, 0.75, na.rm = TRUE)
    IQR_value <- Q3 - Q1
    lower_bound <- Q1 - 0.5 * IQR_value  # Adjusted threshold
    upper_bound <- Q3 + 0.5 * IQR_value
    is_outlier <- (values < lower_bound) | (values > upper_bound)

  # Modified Z-score method: More robust than standard Z-score, using the median absolute deviation (MAD)
  } else if (method == "modified_z") {
    median_value <- median(values, na.rm = TRUE)
    mad_value <- mad(values, na.rm = TRUE)
    modified_z_scores <- 0.6745 * (values - median_value) / mad_value
    is_outlier <- abs(modified_z_scores) > 1.5  # Threshold for Modified Z-score method

  # Dixon’s Q-test: Detects outliers at the extremes of a sorted data set
  } else if (method == "dixon") {
    sorted_values <- sort(values)

    # If all values are the same, there are no outliers
    if (sorted_values[n] == sorted_values[1]) {
      return(data.frame(value = values, outlier = FALSE))  
    } 

    # Calculate Q-statistics for smallest and largest values
    Q_min <- (sorted_values[2] - sorted_values[1]) / (sorted_values[n] - sorted_values[1])
    Q_max <- (sorted_values[n] - sorted_values[n - 1]) / (sorted_values[n] - sorted_values[1])
    critical_Q <- 0.941  # 95% confidence level

    # Mark outliers based on Q-statistic threshold
    is_outlier[values == sorted_values[1]] <- Q_min > critical_Q
    is_outlier[values == sorted_values[n]] <- Q_max > critical_Q
  }

  # Return a dataframe with values and corresponding outlier labels (TRUE/FALSE)
  return(data.frame(value = values, outlier = is_outlier))
}

```


```{r}
example_objects <- c("MAR1_15", "MAR1_16", "MAR1_26", "MAR1_3", "MAR1_34", 
            "MAR1_39", "MAR1_42", "MAR1_45", "MAR1_5", "MAR2_0", "MAR2_1", "MAR2_17", 
            "MAR2_214", "MAR2_25", "MAR2_36", "MAR2_39", "MAR2_47", "MAR3_10", "MAR3_100", 
            "MAR3_114", "MAR3_12", "MAR3_201", "MAR3_209", "MAR3_215", "MAR3_22", "MAR3_28")  # List of specific objects to analyze

example_element <- "Zr"  # Element of interest for outlier detection

# Outlier detection for each object using multiple methods
outlier_results <- cbind(protocol, RFA.val_cal) |>  # Combine protocol and calibration data
  filter(Object != "Standart-TS") |>  # Exclude reference/calibration samples
  filter(Object %in% example_objects) |>  # Filter only selected objects
  dplyr::select(Object, all_of(example_element)) |>  # Select Object and the specified element
  tidyr::unnest(cols = all_of(example_element)) |>  # Expand nested values into separate rows
  group_by(Object) |>  # Group by object to apply outlier detection separately
  summarise(
    iqr = list(detect_outliers(get(example_element), "iqr")),  # Apply IQR method
    z = list(detect_outliers(get(example_element), "z")),  # Apply Z-score method
    dixon = list(detect_outliers(get(example_element), "dixon")),  # Apply Dixon's Q-test
    mod_z = list(detect_outliers(get(example_element), "modified_z"))  # Apply Modified Z-score
  ) |> 
  tidyr::pivot_longer(cols = c(iqr, z, dixon, mod_z), names_to = "method", values_to = "outlier_data") |>  # Convert to long format
  tidyr::unnest(outlier_data)  # Expand list columns into separate rows

```


```{r}
ggplot(outlier_results, aes(x = Object, y = value, color = outlier)) +
  geom_point(size = 1) +  # Points with a custom size
  facet_wrap(~ method) +  # Facet by 'method' for separate plots
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +  # Custom colors for outliers
  labs(
    title = paste("Outlier Detection for", example_element),  # Title with dynamic element
    x = "Object",  # Label for the x-axis
    y = paste("Measurement for", example_element),  # Dynamic label for y-axis
    color = "Outlier?"  # Label for the color legend
  ) +
  theme_minimal() +  # Use minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


```



## quality mean
### new
```{r}
RFA.sd_cl |> 
  pivot_longer(low.error.elements_15, names_to = "Element", values_to = "mean") |>  # Pivot to long format
  group_by(Element) |>  # Group by 'Element'
  summarise(mean = mean(mean, na.rm = TRUE)) |>  # Calculate mean for each element
  arrange(-mean)  # Arrange elements by descending mean

low.sd.elements.cl_10 <- RFA.sd_cl |> 
  pivot_longer(low.error.elements_15, names_to = "Element", values_to = "mean") |>  # Pivot to long format
  group_by(Element) |>  # Group by 'Element'
  summarise(mean = mean(mean, na.rm = TRUE)) |>  # Calculate mean for each element
  filter(mean < 10) |>  # Filter elements with mean less than 10
  pull(Element)  # Extract 'Element' names

low.sd.elements.cl_15 <- RFA.sd_cl |> 
  pivot_longer(low.error.elements_15, names_to = "Element", values_to = "mean") |>  # Pivot to long format
  group_by(Element) |>  # Group by 'Element'
  summarise(mean = mean(mean, na.rm = TRUE)) |>  # Calculate mean for each element
  filter(mean < 15) |>  # Filter elements with mean less than 15
  pull(Element)  # Extract 'Element' names

low.sd.elements.cl_20 <- RFA.sd_cl |> 
  pivot_longer(low.error.elements_15, names_to = "Element", values_to = "mean") |>  # Pivot to long format
  group_by(Element) |>  # Group by 'Element'
  summarise(mean = mean(mean, na.rm = TRUE)) |>  # Calculate mean for each element
  filter(mean < 20) |>  # Filter elements with mean less than 20
  pull(Element)  # Extract 'Element' names

```

```{r}
# Ensure RFA.sd_cl is in a tidy format (long format) for ggplot
RFA.sd_long <- RFA.sd_cl %>%
  dplyr::select(any_of(all_elements)) |>  # Select columns based on 'all_elements'
  pivot_longer(cols = everything(), names_to = "Element", values_to = "Sd") %>%  # Pivot to long format
  filter(!is.na(Sd))  # Remove NA values

# Create the boxplot
ggplot(RFA.sd_long, aes(x = Element, y = Sd)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +  # Boxplot with custom fill color
  labs(
    x = "Element",  # x-axis label
    y = "Relative Error (%)",  # y-axis label
    title = "Distribution of Relative Errors by Element"  # Plot title
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

# Ensure RFA.sd_cl is in a tidy format (long format) for ggplot, excluding 'Cr'
RFA.sd_long <- RFA.sd_cl %>%
  dplyr::select(any_of(all_elements)) |>  # Select columns based on 'all_elements'
  dplyr::select(-Cr) |>  # Exclude the 'Cr' column
  pivot_longer(cols = everything(), names_to = "Element", values_to = "Sd") %>%  # Pivot to long format
  filter(!is.na(Sd))  # Remove NA values

# Create the boxplot
ggplot(RFA.sd_long, aes(x = Element, y = Sd)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +  # Boxplot with custom fill color
  labs(
    x = "Element",  # x-axis label
    y = "Relative Error (%)",  # y-axis label
    title = "Distribution of Relative Errors by Element"  # Plot title
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

```


## modified z as clean data
```{r}
RFA.val_mean <- RFA.val_dix # putting the means that were cleanded with Dixons method as the data used in further processes
```



#elements
## main elements
```{r}
main_elements <- c("Si", "Al", "Ca", "K", "Fe", "Ti", "P", "Mn")  # Define the main elements for selection

RFA.ME <- RFA.val_mean |> 
  dplyr::select(all_of(main_elements)) |>  # Select columns for the main elements
  mutate(Si = Si*2.1392,  # Adjust Si values
         Al = Al*1.8895,  # Adjust Al values
         Ca = Ca*1.3992,  # Adjust Ca values
         K = K*1.2046,  # Adjust K values
         Fe = Fe*1.4297,  # Adjust Fe values
         # S = S*2.4972,  # Commented out S adjustment
         Ti = Ti*1.6681,  # Adjust Ti values
         P = 2.2916,  # Set P to a fixed value
         Mn = 1.5825  # Set Mn to a fixed value
  )

df_long <- RFA.ME %>%
  rownames_to_column(var = "row_id") %>%  # Convert row names to a column called 'row_id'
  pivot_longer(-row_id, names_to = "variable", values_to = "value")  # Pivot data to long format

# Berechnen Sie die Summe jeder Zeile und fügen Sie sie hinzu
df_long <- df_long %>%
  group_by(row_id) %>%  # Group by 'row_id'
  mutate(row_sum = sum(value)) %>%  # Calculate row sum
  ungroup()  # Ungroup after summing

# Berechnen Sie die Prozente
df_long <- df_long %>%
  mutate(percent = (value / row_sum) * 100)  # Calculate percentage of each value

# Konvertieren Sie zurück in das breite Format
RFA.ME <- df_long %>%
  dplyr::select(row_id, variable, percent) %>%  # Select 'row_id', 'variable', and 'percent' columns
  pivot_wider(names_from = "variable", values_from = "percent") %>%  # Pivot data back to wide format
  column_to_rownames(var = "row_id")  # Convert 'row_id' column back to row names

```


## new data
```{r}

RFA.val <- RFA.val_mean  # Assign RFA.val_mean to RFA.val

RFA.val <- RFA.val |> 
  dplyr::select(any_of(all_elements))  # Select only the columns in 'all_elements'

all_elements <- RFA.val |> 
  rename(Al2O3 = Al, Fe2O3 = Fe, MnO = Mn, SiO2 = Si) |>  # Rename elements for consistency
  colnames()  # Get column names after renaming

RFA.val <- RFA.val |> 
  dplyr::select(-all_of(main_elements)) |>  # Exclude 'main_elements' columns
  cbind(RFA.ME) |>  # Add the 'RFA.ME' data frame
  dplyr::select(any_of(colnames(protocol)), all_of(main_elements), everything())  # Reorder columns to match protocol and main_elements

low.sd.elements.cl_10 <- RFA.val |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select columns in 'low.sd.elements.cl_10'
  rename(Al2O3 = Al, Fe2O3 = Fe, MnO = Mn, SiO2 = Si) |>  # Rename elements for consistency
  colnames()  # Get column names after renaming

RFA.val <- RFA.val |> 
  rename(Al2O3 = Al, Fe2O3 = Fe, MnO = Mn, SiO2 = Si)  # Rename elements for consistency

```


# Standardvalue
```{r}
RFA.std <- RFA.val |>  
  dplyr::select(any_of(all_elements)) |>  # Select columns from 'all_elements'
  mutate_all(~ (. - mean(.)) / sd(.))  # Standardize each column by subtracting the mean and dividing by the standard deviation


```


# final RFA.val
```{r}
protocol_mean <- protocol |> 
  group_by(Object) |>  # Group data by 'Object'
  summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE)),  # Calculate mean for numeric columns, ignoring NAs
            across(where(is.character), first))  # Take the first value for character columns

RFA.val <- protocol_mean |> 
  cbind(RFA.val) |>  # Combine 'protocol_mean' with 'RFA.val' by column
  remove_rownames()  # Remove row names

RFA.std <- protocol_mean |> 
  cbind(RFA.std) |>  # Combine 'protocol_mean' with 'RFA.std' by column
  remove_rownames()  # Remove row names

```


```{r}
RFA.val |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select columns in 'low.sd.elements.cl_10'
  summarise(across(everything(), ~ (sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE)) * 100))  # Calculate coefficient of variation (CV) for each selected column

```






# Marandet with attributes
```{r}
# dput(pottery2, file = "mar_pot_attr.R")  # (Commented out) Save 'pottery2' to a file

pot_attr <- read.csv2(here("data/pottery_attr_mar.csv"))  # Read the pottery attributes from the CSV file

mar_attr <- RFA.val |> 
  filter(Site == "Marandet") |>  # Filter rows where 'Site' is "Marandet"
  left_join(pot_attr, by = c("Object" = "RFAID")) |>  # Join 'RFA.val' with 'pot_attr' based on 'Object' and 'RFAID'
  filter(!is.na(Slip))  # Filter rows where 'Slip' is not NA

mar_attr <- RFA.std |> 
  filter(Site == "Marandet") |>  # Filter rows where 'Site' is "Marandet"
  left_join(pot_attr, by = c("Object" = "RFAID")) |>  # Join 'RFA.std' with 'pot_attr' based on 'Object' and 'RFAID'
  filter(!is.na(Slip))  # Filter rows where 'Slip' is not NA

pot_attr  # Display the 'pot_attr' data frame

```

```{r}

sgrf_mar <- c("MAR3_208", "MAR2_39", "MAR1_4", "MAR3_209", "MAR3_28")  # Define the list of objects with sgraffito decoration

rfa_mar <- mar_attr |> 
  unite(Rim, Collar, Lip) |>  # Combine 'Rim', 'Collar', and 'Lip' columns into a single 'Rim' column
  # dplyr::select(Object, all_elements, X:Angle.fc) |>  # (Commented out) Select specific columns (if needed)
  mutate(
    JAS_decor = case_when(  # Create new column 'JAS_decor' based on conditions
        Object %in% sgrf_mar ~ "Sgraffito",  # Assign "Sgraffito" for objects in 'sgrf_mar'
        Object == "MAR_204" ~ "Wheel-thrown",  # Assign "Wheel-thrown" for a specific object
        grepl("^Undecorated", JAS_decor) ~ "Undecorated",  # Assign "Undecorated" for 'JAS_decor' starting with "Undecorated"
        grepl("^Folded_strip_roulette", JAS_decor) ~ "Folded_strip_roulette",  # Assign "Folded_strip_roulette"
        grepl("^Mat", JAS_decor) ~ "Mat",  # Assign "Mat" for 'JAS_decor' starting with "Mat"
        TRUE ~ "Other decor"  # Default to "Other decor" for any other cases
    )
  ) |> 
  column_to_rownames("Object")  # Set 'Object' column as row names


  
```



##decor and pca
```{r}
pca_mar <- rfa_mar |> 
  dplyr::select(Rb, Y, Th, Fe2O3, Zr, Sr) |>  # Select variables for PCA
  PCA()  # Perform Principal Component Analysis (PCA)

# Plot PCA results for first and second principal components
pca_mar_dec_ind_1_2 <- fviz_pca_ind(pca_mar,
             geom.ind = "point",  # Show points only (not "text")
             col.ind = rfa_mar$JAS_decor,  # Color points by 'JAS_decor' groups
             pointshape = 16,  # Set point shape
             palette = c("#E69F00", "#56B4E9", "#009E73", 
                         "#F0E442", "black", "#CC79A7"),  # Custom color palette
             legend.title = "Groups",  # Set legend title
             mean.point = FALSE  # Do not show mean points
             )

# Plot PCA results for first and third principal components
pca_mar_dec_ind_1_3 <- fviz_pca_ind(pca_mar,
             axes = c(1, 3),  # Plot first and third principal components
             geom.ind = "point",  # Show points only (not "text")
             col.ind = rfa_mar$JAS_decor,  # Color points by 'JAS_decor' groups
             pointshape = 16,  # Set point shape
             palette = c("#E69F00", "#56B4E9", "#009E73", 
                         "#F0E442", "black", "#CC79A7"),  # Custom color palette
             legend.title = "Groups",  # Set legend title
             mean.point = FALSE  # Do not show mean points
             )

# Plot PCA results colored by 'Rim' variable in 'mar_attr'
fviz_pca_ind(pca_mar,
             geom.ind = "point",  # Show points only (not "text")
             col.ind = mar_attr$Rim,  # Color points by 'Rim'
             pointshape = 16,  # Set point shape
             palette = c("#E69F00", "#56B4E9", "#009E73", 
                         "#F0E442", "black", "#CC79A7"),  # Custom color palette
             legend.title = "Groups"  # Set legend title
             )

# Plot PCA results colored by 'Rim' variable in 'rfa_mar'
fviz_pca_ind(pca_mar,
             geom.ind = "point",  # Show points only (not "text")
             col.ind = rfa_mar$Rim,  # Color points by 'Rim'
             pointshape = 16,  # Set point shape
             legend.title = "Groups"  # Set legend title
             )

# Plot PCA variable correlations for first and second principal components
pca_mar_var_1_2 <- fviz_pca_var(pca_mar, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Set color gradient for variables
             repel = TRUE  # Avoid text overlapping
             )

# Plot PCA variable correlations for first and third principal components
pca_mar_var_1_3 <- fviz_pca_var(pca_mar, axes = c(1, 3), col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Set color gradient for variables
             repel = TRUE  # Avoid text overlapping
             )


```



```{r}
# Display PCA plot for the first and second principal components
pca_mar_dec_ind_1_2

# Display PCA plot for the first and third principal components
pca_mar_dec_ind_1_3

# Save the PCA plot for the first and second principal components as a .jpeg file
ggsave(here("results/plots/pca_mar_dec_ind_1_2.jpeg"), 
       plot = pca_mar_dec_ind_1_2, 
       device = "jpeg", 
       dpi = 300,  # Set resolution to 300 dpi
       width = 8,  # Set plot width to 8 inches
       height = 6)  # Set plot height to 6 inches

# Save the PCA plot for the first and third principal components as a .jpeg file
ggsave(here("results/plots/pca_mar_dec_ind_1_3.jpeg"), 
       plot = pca_mar_dec_ind_1_3, 
       device = "jpeg", 
       dpi = 300,  # Set resolution to 300 dpi
       width = 8,  # Set plot width to 8 inches
       height = 6)  # Set plot height to 6 inches

# Load cowplot library for arranging multiple plots
library(cowplot)

# Combine two PCA variable plots into one grid (1 row, 2 columns)
pca_mar_var <- plot_grid(pca_mar_var_1_2, pca_mar_var_1_3, ncol = 2)

# Display the combined PCA variable plot
pca_mar_var

# Save the combined PCA variable plot as a .jpeg file
ggsave(here("results/plots/pca_mar_var.jpeg"), 
       plot = pca_mar_var, 
       device = "jpeg", 
       dpi = 300,  # Set resolution to 300 dpi
       width = 8,  # Set plot width to 8 inches
       height = 6)  # Set plot height to 6 inches

```



# malhanobis function
```{r}
# Function to detect Mahalanobis outliers
detect_mahalanobis_outliers <- function(data, pca = NULL, quantile_threshold = 0.5) {
  # If a PCA object is provided, use its coordinates as the data basis
  if (!is.null(pca)) {
    data <- pca$ind$coord  # Default is to use PCA coordinates from FactoMineR
  }
  
  # Remove columns with zero variance to keep covariance matrix invertible
  # data <- data[, apply(data, 2, var) > 0, drop = FALSE]
  
  # Access the coordinates from the PCA object (if provided)
  data <- data$ind$coord
  
  # Calculate the Mahalanobis distance
  center <- colMeans(data, na.rm = TRUE)  # Calculate the mean for each variable
  cov_matrix <- cov(data, use = "complete.obs")  # Calculate the covariance matrix
  mahal_dist <- mahalanobis(data, center, cov_matrix)  # Calculate Mahalanobis distance
  
  # Set the Chi-square threshold based on the quantile provided
  cutoff <- qchisq(quantile_threshold, df = ncol(data))  # Use degrees of freedom = number of variables
  
  # Identify outliers (Mahalanobis distance greater than cutoff)
  outliers <- which(mahal_dist > cutoff)
  
  # Return the indices of outliers
  return(outliers)
}

# Perform PCA on selected variables from mar_pca dataset
mar_pca <- rfa_mar |> 
  dplyr::select(Rb, Sr, Y, Zn, Th) |> 
  PCA()

# Detect Mahalanobis outliers using the custom function
outliers <- detect_mahalanobis_outliers(mar_pca)

```

# RFA.std as final version
```{r}
RFA.fin <- RFA.std # Making the standardized RFA values the basis for further analysis

```


# removing outliers from regional groups
## Chad
### Lake_Chad_Northwest

```{r}
# Filter and clean the data for the Lake Chad Northwest region and specific objects
chad_nw1 <- RFA.fin |> 
  filter(Region == "Lake_Chad_Northwest" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |> 
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the selected elements from the filtered data
chad_nw_pca <- chad_nw1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_nw_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers
length(outliers)  # Output the number of detected outliers

```

```{r}
# PCA visualization with outliers highlighted
fviz_pca_ind(chad_nw_pca, 
             label = "none",  # Do not display labels
             habillage = 1:nrow(chad_nw1) %in% outliers,  # Highlight outliers (True for outliers)
             addEllipses = TRUE,  # Add concentration ellipses
             palette = c("black", "red"))  # Color points based on whether they are outliers

# PCA visualization with outliers highlighted, axes 3 and 4
fviz_pca_ind(chad_nw_pca, 
             axes = c(3,4),  # Use the 3rd and 4th principal components
             label = "none",  # Do not display labels
             habillage = 1:nrow(chad_nw1) %in% outliers,  # Highlight outliers (True for outliers)
             addEllipses = TRUE,  # Add concentration ellipses
             palette = c("black", "red"))  # Color points based on whether they are outliers

# PCA visualization without outliers highlighted, axes 1 and 2
fviz_pca_ind(chad_nw_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components

# PCA variable visualization, axes 1 and 2
fviz_pca_var(chad_nw_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components

# PCA visualization without outliers highlighted, axes 3 and 4
fviz_pca_ind(chad_nw_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components

# PCA variable visualization, axes 3 and 4
fviz_pca_var(chad_nw_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components

# Store outliers' indices
outliers_chad_nw <- outliers

```

### Lake_Chad_west


```{r}
# Filter and clean the data for the Lake Chad West region and specific objects
chad_w1 <- RFA.fin |> 
  filter(Region == "Lake_Chad_West" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |> 
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the selected elements from the filtered data
chad_w_pca <- chad_w1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_w_pca)  # Call the function to detect outliers

# Output the indices of the detected outliers
outliers  

```

```{r}
# Visualize PCA with highlighting of outliers
fviz_pca_ind(chad_w_pca, 
             label = "none",  # No labels displayed for the points
             habillage = 1:nrow(chad_w1) %in% outliers,  # Color points based on whether they are outliers
             palette = c("black", "red"))  # Black for non-outliers, red for outliers

# Visualize PCA with highlighting of outliers on axes 3 and 4
fviz_pca_ind(chad_w_pca, 
             axes = c(3,4),  # Use axes 3 and 4 for the PCA plot
             label = "none",  # No labels displayed for the points
             habillage = 1:nrow(chad_w1) %in% outliers,  # Color points based on whether they are outliers
             palette = c("black", "red"))  # Black for non-outliers, red for outliers

# Visualize PCA without highlighting outliers, using axes 1 and 2
fviz_pca_ind(chad_w_pca, 
             axes = c(1,2))  # Use axes 1 and 2 for the PCA plot

# Visualize PCA variable contributions, highlighting the relationship between variables on axes 1 and 2
fviz_pca_var(chad_w_pca, 
             axes = c(1,2))  # Use axes 1 and 2 for the PCA variable plot

# Visualize PCA without highlighting outliers, using axes 3 and 4
fviz_pca_ind(chad_w_pca, 
             axes = c(3,4))  # Use axes 3 and 4 for the PCA plot

# Visualize PCA variable contributions, highlighting the relationship between variables on axes 3 and 4
fviz_pca_var(chad_w_pca, 
             axes = c(3,4))  # Use axes 3 and 4 for the PCA variable plot

# Store the detected outliers for further analysis
outliers_chad_w <- outliers  # Save the outliers detected in the PCA

```
### Lake Chad East

```{r}
# Filter the data for the Lake Chad East region and exclude certain objects
chad_e1 <- RFA.fin |> 
  filter((Region == "Lake_Chad_East") | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Filter by region and specific objects
  filter(!Object %in% c("KAM_4", "TIE_100", "TIE_101", "TIE_13", "TIE_15"))|>  # Exclude specific objects
  filter(!grepl("^TIE2", Object)) |>  # Exclude objects with names starting with "TIE2"
  remove_rownames() |>  # Remove row names from the data
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the filtered data, selecting the elements in 'low.sd.elements.cl_10'
chad_e_pca <- chad_e1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_e_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers

```

```{r}

# PCA with highlighting of the outliers (showing points)
fviz_pca_ind(chad_e_pca, 
             label = "none",  # Do not display labels
             habillage = 1:nrow(chad_e1) %in% outliers,  # Color the points based on whether they are outliers
             addEllipses = TRUE,  # Add ellipses around the groups
             palette = c("black", "red"))  # Color the points black for non-outliers and red for outliers

# PCA with highlighting of the outliers (using axes 3 and 4)
fviz_pca_ind(chad_e_pca, 
             axes = c(3,4),  # Display the PCA plot using axes 3 and 4
             label = "none",  # Do not display labels
             habillage = 1:nrow(chad_e1) %in% outliers,  # Color the points based on whether they are outliers
             addEllipses = TRUE,  # Add ellipses around the groups
             palette = c("black", "red"))  # Color the points black for non-outliers and red for outliers

# PCA with highlighting of the outliers (using axes 1 and 2)
fviz_pca_ind(chad_e_pca, 
             axes = c(1,2))  # Display the PCA plot using axes 1 and 2

# PCA variable plot with highlighting of the outliers (using axes 1 and 2)
fviz_pca_var(chad_e_pca, 
             axes = c(1,2))  # Display the PCA variable plot using axes 1 and 2

# PCA with highlighting of the outliers (using axes 3 and 4)
fviz_pca_ind(chad_e_pca, 
             axes = c(3,4))  # Display the PCA plot using axes 3 and 4

# PCA variable plot with highlighting of the outliers (using axes 3 and 4)
fviz_pca_var(chad_e_pca, 
             axes = c(3,4))  # Display the PCA variable plot using axes 3 and 4

# Store the outliers for further use
outliers_chad_e <- outliers  # Save the detected outliers in a new variable

```


### Lake Chad South

```{r}
# Filter and clean the data for the Lake Chad South region and specific objects
chad_s1 <- RFA.fin |> 
  filter(Region == "Lake_Chad_South" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Filter for the specific region or objects
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the selected elements from the filtered data
chad_s_pca <- chad_s1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements from 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_s_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers

```

```{r}
# PCA with highlighting of outliers (black = not outlier, red = outlier)
fviz_pca_ind(chad_s_pca, 
             label = "none",  # Do not label the points
             habillage = 1:nrow(chad_s1) %in% outliers,  # Highlight outliers
             addEllipses = TRUE,  # Add ellipses for the groups
             palette = c("black", "red"))  # Color scheme for points (black for normal, red for outliers)

# PCA with highlighting of outliers, using axes 3 and 4
fviz_pca_ind(chad_s_pca, 
             axes = c(3,4),  # Use the 3rd and 4th principal components for plotting
             label = "none",  # Do not label the points
             habillage = 1:nrow(chad_s1) %in% outliers,  # Highlight outliers
             addEllipses = TRUE,  # Add ellipses for the groups
             palette = c("black", "red"))  # Color scheme for points (black for normal, red for outliers)

# PCA with highlighting of outliers, using axes 1 and 2
fviz_pca_ind(chad_s_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components for plotting

# PCA variable plot with highlighting of outliers, using axes 1 and 2
fviz_pca_var(chad_s_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components for variable plotting

# PCA with highlighting of outliers, using axes 3 and 4
fviz_pca_ind(chad_s_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components for plotting

# PCA variable plot with highlighting of outliers, using axes 3 and 4
fviz_pca_var(chad_s_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components for variable plotting

# Store the detected outliers in a variable for later use
outliers_chad_s <- outliers  # Store the detected outliers for the Lake Chad South region


```


### Lake Chad Southeast

```{r}
# Filter and clean the data for the Lake Chad Southeast region and specific objects
chad_se1 <- RFA.fin |> 
  filter(Region == "Lake_Chad_Southeast" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |> 
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the selected elements from the filtered data
chad_se_pca <- chad_se1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_se_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers

```

```{r}
# PCA with highlighting of outliers
fviz_pca_ind(chad_se_pca, 
             label = "none",  # No labels on the points
             habillage = 1:nrow(chad_se1) %in% outliers,  # Color points by whether they are outliers
             addEllipses = TRUE,  # Add ellipses to visualize clusters
             palette = c("black", "red"))  # Black for non-outliers, red for outliers

# PCA with highlighting of outliers (axes 3 and 4)
fviz_pca_ind(chad_se_pca, 
             axes = c(3,4),  # Use the third and fourth principal components
             label = "none",  # No labels on the points
             habillage = 1:nrow(chad_se1) %in% outliers,  # Color points by whether they are outliers
             addEllipses = TRUE,  # Add ellipses to visualize clusters
             palette = c("black", "red"))  # Black for non-outliers, red for outliers

# PCA with highlighting of outliers (axes 1 and 2)
fviz_pca_ind(chad_se_pca, 
             axes = c(1,2))  # Use the first and second principal components

# PCA with variable contributions (axes 1 and 2)
fviz_pca_var(chad_se_pca, 
             axes = c(1,2))  # Visualize variable contributions on axes 1 and 2

# PCA with highlighting of outliers (axes 3 and 4)
fviz_pca_ind(chad_s_pca, 
             axes = c(3,4))  # Visualize PCA with axes 3 and 4

# PCA with variable contributions (axes 3 and 4)
fviz_pca_var(chad_se_pca, 
             axes = c(3,4))  # Visualize variable contributions on axes 3 and 4

# Store the detected outliers for Lake Chad Southeast region
outliers_chad_se <- outliers  # Save the outliers for later use

```


### Lake Chad Southwest

```{r}
# Filter and clean the data for the Lake Chad Southwest region and specific objects
chad_sw1 <- RFA.fin |> 
  filter(Region == "Lake_Chad_Southwest" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Filter for the region and specific objects
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set the "Object" column as row names

# Perform PCA on the selected elements from the filtered data
chad_sw_pca <- chad_sw1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(chad_sw_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers

```

```{r}
# PCA with highlighting of outliers (2D plot of the first two principal components)
fviz_pca_ind(chad_sw_pca, 
             label = "none",  # No labels on the points
             habillage = 1:nrow(chad_sw1) %in% outliers,  # Color points based on whether they are outliers
             addEllipses = TRUE,  # Add confidence ellipses for each group
             palette = c("black", "red"))  # Color palette (black for non-outliers, red for outliers)

# PCA with highlighting of outliers (2D plot of the third and fourth principal components)
fviz_pca_ind(chad_sw_pca, 
             axes = c(3,4),  # Use the 3rd and 4th principal components for the axes
             label = "none",  # No labels on the points
             habillage = 1:nrow(chad_sw1) %in% outliers,  # Color points based on whether they are outliers
             addEllipses = TRUE,  # Add confidence ellipses for each group
             palette = c("black", "red"))  # Color palette (black for non-outliers, red for outliers)

# PCA with highlighting of outliers (2D plot of the first two principal components)
fviz_pca_ind(chad_sw_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components for the axes

# PCA with variable contributions (2D plot of the first two principal components)
fviz_pca_var(chad_sw_pca, 
             axes = c(1,2))  # Use the 1st and 2nd principal components for the axes

# PCA with highlighting of outliers (2D plot of the third and fourth principal components)
fviz_pca_ind(chad_s_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components for the axes

# PCA with variable contributions (2D plot of the third and fourth principal components)
fviz_pca_var(chad_sw_pca, 
             axes = c(3,4))  # Use the 3rd and 4th principal components for the axes

# Store outliers for later reference
outliers_chad_sw <- outliers

```


## Essouk

```{r}
# Filter and clean the data for the Essouk region, including specific objects
ess1 <- RFA.fin |> 
  filter(Region == "Essouk" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Filter for the Essouk region or specific objects
  remove_rownames() |>  # Remove row names to avoid issues with row-based operations
  filter(!(Object %in% c("ESS_23","ESS_24","ESS_25","ESS_26","ESS_27","ESS_28","ESS_29"))) |>  # Remove specific unwanted objects
  column_to_rownames("Object")  # Set the "Object" column as the row names for the data

# Perform PCA on the selected elements from the filtered data
ess_pca <- ess1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements defined in 'low.sd.elements

```

```{r}

# PCA visualization with highlighting of outliers
fviz_pca_ind(ess_pca, 
             label = "none",  # Do not display labels for individual points
             habillage = 1:nrow(ess1) %in% outliers,  # Highlight outliers in red, others in black
             addEllipses = TRUE,  # Add confidence ellipses around groups
             palette = c("black", "red"))  # Color palette (black for normal points, red for outliers)

# PCA visualization on axes 3 and 4 with outlier highlighting
fviz_pca_ind(ess_pca, 
             axes = c(3,4),  # Use the 3rd and 4th principal components as the axes
             label = "none",  # No labels for individual points
             habillage = 1:nrow(ess1) %in% outliers,  # Highlight outliers
             addEllipses = TRUE,  # Add ellipses
             palette = c("black", "red"))  # Color scheme for normal points and outliers

# PCA visualization on the first and second principal components
fviz_pca_ind(ess_pca, 
             axes = c(1,2))  # Show the plot using the first and second principal components

# PCA visualization of variable contributions (for first and second components)
fviz_pca_var(ess_pca, 
             axes = c(1,2))  # Show variable contributions to the first and second components

# PCA visualization on axes 3 and 4 (again for individual points)
fviz_pca_ind(ess_pca, 
             axes = c(3,4))  # Use the third and fourth components

# PCA visualization of variable contributions (on axes 3 and 4)
fviz_pca_var(chad_se_pca, 
             axes = c(3,4))  # Plot variable contributions for axes 3 and 4 of a different PCA (chad_se_pca)

# Save the outliers detected in the PCA
outliers_ess <- outliers  # Store the detected outliers for the Essouk region

```

## Niger_Niamey
```{r}
# Filter and clean the data for the Karey_Gorou site and specific objects
nig_nia1 <- RFA.fin |> 
  filter(Site == "Karey_Gorou" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Select rows with 'Karey_Gorou' site or specific objects
  remove_rownames() |>  # Remove the row names from the data
  column_to_rownames("Object")  # Set the 'Object' column as the row names

# Perform PCA on the selected elements from the filtered data
nig_nia_pca <- nig_nia1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements specified in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(nig_nia_pca)  # Call the function to detect outliers
outliers  # Output the indices of the outliers

```

```{r}
# PCA with highlighting of outliers
fviz_pca_ind(nig_nia_pca, 
             label = "none",  # Remove labels for individual points
             habillage = 1:nrow(nig_nia1) %in% outliers,  # Highlight outliers in the plot
             addEllipses = TRUE,  # Add ellipses around clusters
             palette = c("black", "red"))  # Use black for non-outliers and red for outliers

# PCA with highlighting of outliers, using axes 3 and 4
fviz_pca_ind(nig_nia_pca, 
             axes = c(3,4),  # Use the third and fourth principal components
             label = "none",  # Remove labels for individual points
             habillage = 1:nrow(nig_nia1) %in% outliers,  # Highlight outliers
             addEllipses = TRUE,  # Add ellipses around clusters
             palette = c("black", "red"))  # Use black for non-outliers and red for outliers

# PCA plot showing axes 1 and 2
fviz_pca_ind(nig_nia_pca, 
             axes = c(1,2))  # Use the first and second principal components

# PCA plot showing variable contributions on axes 1 and 2
fviz_pca_var(nig_nia_pca, 
             axes = c(1,2))  # Show variable contributions on axes 1 and 2

# PCA plot showing axes 3 and 4 for the individual points
fviz_pca_ind(nig_nia_pca, 
             axes = c(3,4))  # Use the third and fourth principal components for the individual points

# PCA plot showing variable contributions on axes 3 and 4
fviz_pca_var(nig_nia_pca, 
             axes = c(3,4))  # Show variable contributions on axes 3 and 4

# Store the outliers detected in the previous analysis
outliers_nig_nia <- outliers  # Store the detected outliers for further use
```

## Marandet
```{r}
# Define a vector of objects with atypical types to be excluded from the analysis
untypical_types_mar <- c("MAR1_15", "MAR1_16", "MAR1_26", "MAR1_3", 
            "MAR1_34", "MAR1_39", "MAR1_42", "MAR1_45", "MAR1_5", "MAR2_1", "MAR2_17", 
            "MAR2_214", "MAR2_25", "MAR2_36", "MAR2_39", "MAR2_47", "MAR3_10", 
            "MAR3_100", "MAR3_114", "MAR3_12", "MAR3_201", "MAR3_209", "MAR3_215", 
            "MAR3_22", "MAR3_28", "MAR1_18", "MAR1_24", "MAR1_4", "MAR1_7", 
            "MAR2_203", "MAR2_3", "MAR2_52", "MAR3_208", "MAR3_3", "MAR3_34", "MAR_204", "MAR3_84", "MAR2_0", "MAR_1001", "MAR_1002", "MAR_1003")

# Filter the data: exclude rows with objects in the 'untypical_types_mar' list 
# and select data from the "Marandet" region or specific objects
mar1 <- RFA.fin |> 
  filter(!(Object %in% c(untypical_types_mar))) |>  # Remove rows with atypical object types
  filter(Region == "Marandet" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |>  # Keep only specified regions or objects
  column_to_rownames("Object")  # Set "Object" column as row names

# Perform PCA on the filtered data, using only the specified elements
mar_pca <- mar1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the elements that are in 'low.sd.elements.cl_10'
  PCA()  # Perform PCA analysis

# Detect Mahalanobis outliers based on the PCA result
outliers <- detect_mahalanobis_outliers(mar_pca)  # Call the function to detect outliers

# Output the indices of the detected outliers
outliers  # Display the detected outliers

```

```{r}
# PCA with highlighting of outliers (default axes: PC1 and PC2)
fviz_pca_ind(mar_pca, 
             label = "none", 
             habillage = 1:nrow(mar1) %in% outliers,  # Highlight outliers in the data
             addEllipses = TRUE,  # Add ellipses for groupings
             palette = c("black", "red"))  # Red for outliers, black for the rest

# PCA with highlighting of outliers (axes: PC3 and PC4)
fviz_pca_ind(mar_pca, 
             axes = c(3,4),  # Use third and fourth principal components
             label = "none", 
             habillage = 1:nrow(mar1) %in% outliers,  # Highlight outliers in the data
             addEllipses = TRUE,  # Add ellipses for groupings
             palette = c("black", "red"))  # Red for outliers, black for the rest

# PCA with highlighting of outliers (axes: PC1 and PC2)
fviz_pca_ind(mar_pca, 
             axes = c(1,2))  # Use first and second principal components

# PCA variable plot (axes: PC1 and PC2)
fviz_pca_var(mar_pca, 
             axes = c(1,2))  # Show how the variables contribute to the first two components

# PCA with highlighting of outliers (axes: PC3 and PC4) for individuals
fviz_pca_ind(mar_pca, 
             axes = c(3,4))  # Use third and fourth principal components for individuals

# PCA variable plot (axes: PC3 and PC4)
fviz_pca_var(mar_pca, 
             axes = c(3,4))  # Show how the variables contribute to the third and fourth components

# Save outliers list for further analysis
outliers_mar <- outliers  # Save the outliers to a variable for later reference


```
## Zinder
```{r}
# Filter the dataset to only include rows for Zinder region or specified objects
zin1 <- RFA.fin |> 
  filter(Region == "Zinder" | Object %in% c("MAR3_54", "ESS_13", "KAG3_5", "TIE_70", "MDA_6")) |> 
  remove_rownames() |>  # Remove row names for clean data
  column_to_rownames("Object")  # Set the "Object" column as row names for the dataset

# Perform PCA on the selected elements
zin_pca <- zin1 |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select elements based on the 'low.sd.elements.cl_10' list
  PCA()  # Perform PCA on the selected data

# Detect Mahalanobis outliers based on PCA results
outliers <- detect_mahalanobis_outliers(zin_pca)  # Identify outliers using the Mahalanobis distance method
outliers  # Output the detected outliers

```

```{r}
# PCA with highlighting of outliers using the first two principal components
fviz_pca_ind(zin_pca, 
             label = "none",  # Do not display labels on the points
             habillage = 1:nrow(zin1) %in% outliers,  # Highlight outliers in red
             addEllipses = TRUE,  # Add ellipses around groups (outliers and others)
             palette = c("black", "red"))  # Set the color palette to black for regular points and red for outliers

# PCA with highlighting of outliers using the third and fourth principal components
fviz_pca_ind(zin_pca, 
             axes = c(3,4),  # Use the 3rd and 4th principal components for visualization
             label = "none", 
             habillage = 1:nrow(zin1) %in% outliers,  # Highlight outliers in red
             addEllipses = TRUE, 
             palette = c("black", "red"))  # Color palette for points

# PCA with highlighting of outliers using the first two principal components (individual points only)
fviz_pca_ind(zin_pca, 
             axes = c(1,2)  # Using the 1st and 2nd principal components
           )

# PCA with highlighting of outliers, showing the contributions of variables
fviz_pca_var(zin_pca, 
             axes = c(1,2)  # Using the 1st and 2nd principal components for variable contributions
           )

# PCA with highlighting of outliers using the third and fourth principal components (individual points only)
fviz_pca_ind(zin_pca, 
             axes = c(3,4)  # Using the 3rd and 4th principal components
           )

# PCA with highlighting of outliers, showing the contributions of variables for the 3rd and 4th principal components
fviz_pca_var(zin_pca, 
             axes = c(3,4)  # Using the 3rd and 4th principal components
           )

# Store the detected outliers for further analysis
outliers_zin <- outliers

```

# list of only reference groups
```{r}
# Combine all detected outliers from various regions and objects into a single vector
outliers_all <- c(outliers_mar, outliers_chad_nw, outliers_chad_e, 
                  outliers_chad_sw, outliers_chad_w, outliers_chad_se, 
                  outliers_chad_s, outliers_nig_nia, outliers_ess, outliers_zin)

# Add additional outlier objects (untypical types and a specific object "WAL_16") to the outlier list
outliers_all <- c(names(outliers_all), untypical_types_mar, "WAL_16")

# Filter the RFA.fin dataset to exclude rows where the 'Object' is in the outliers_all list
RFA.ref <- RFA.fin |> 
  filter(!(Object %in% outliers_all))  # Remove all rows with outliers

```

# discriminatory ref

```{r}
# Filter the RFA.val dataset based on matching Objects in RFA.ref and exclude a specific site ("Birni_Lafyia")
RFA4 <- RFA.val |> 
  filter((Object %in% RFA.ref$Object)) |>  # Keep only the objects present in RFA.ref
  remove_rownames() |>  # Remove row names (if any) in the dataset
  column_to_rownames("Object")  # Set "Object" column as row names for the dataset

# Check for duplicated 'Object' values in RFA3 dataset and list them
RFA3$Object[duplicated(RFA3$Object)]  # This will return duplicated 'Object' values from RFA3

# Check for duplicated 'Object' values in RFA.val dataset and list them
RFA.val$Object[duplicated(RFA.val$Object)]  # This will return duplicated 'Object' values from RFA.val

```

```{r}
# Create a new dataset 'all_dis' by selecting specific columns from 'RFA.ref'
all_dis <- RFA.ref |> 
  dplyr::select(Region, any_of(low.sd.elements.cl_10)) |>  # Select 'Region' column and any elements in 'low.sd.elements.cl_10'
  rename(Prov = Region)  # Rename the 'Region' column to 'Prov'

```

```{r}
# Set the random seed for reproducibility
set.seed(123)

# Create training samples by partitioning 'all_dis$Prov' (the target variable)
# 80% for training data, remaining 20% for test data
training.samples <- all_dis$Prov %>%
  createDataPartition(p = 0.8, list = FALSE)

# Subset the 'all_dis' data to create training and testing datasets
train.data <- all_dis[training.samples, ]  # 80% of data for training
test.data <- all_dis[-training.samples, ]  # 20% of data for testing

# Estimate preprocessing parameters for centering and scaling (standardizing the data)
# Using the training data to estimate the transformation parameters
preproc.param <- train.data %>% 
  preProcess(method = c("center", "scale"))  # Center (mean=0) and scale (standard deviation=1)

# Apply the preprocessing parameters to both training and testing datasets
train.transformed <- preproc.param %>% predict(train.data)  # Apply to training data
test.transformed <- preproc.param %>% predict(test.data)  # Apply to testing data

```


```{r}
# Load the MASS package for Linear Discriminant Analysis (LDA)
library(MASS)

# Fit the Linear Discriminant Analysis (LDA) model
# The target variable is 'Prov', and all other variables are used as predictors
model <- lda(Prov ~ ., data = train.transformed)

# Make predictions on the test data using the trained model
predictions <- model %>% predict(test.transformed)

# Calculate and display the model accuracy by comparing predicted classes to the actual values
accuracy <- mean(predictions$class == test.transformed$Prov)
print(accuracy)

# Display the model summary
model

# Optional: Uncomment the next line to plot the model (if needed)
# plot(model)

# Extract the means of each class (group) for all variables
means <- model$means

# Calculate the absolute differences between consecutive group means for each variable
# 'lag()' function shifts the data by one row, and the differences are calculated
dif <- means %>%
  as.data.frame() %>%
  summarise_all(~ sum(abs(. - lag(.)), na.rm = TRUE)) %>%
  t() %>%
  as.data.frame() %>%
  arrange(-V1)  # Sort by the difference in means

# Print the dataframe showing the differences between group means for each variable
print(dif)

# Check the class type of the 'means' object (should be a matrix or dataframe)
class(means)

# Create a scatter plot with labels showing the relationship between 'Y' and 'Rb', colored by 'Prov'
all_dis %>%
  rownames_to_column("ID") %>%
  ggplot() +
  geom_label(aes(x = Y, y = Rb, colour = Prov, label = ID))

# Create a scatter plot with labels showing the relationship between 'Zn' and 'V', colored by 'Prov'
all_dis %>%
  rownames_to_column("ID") %>%
  ggplot() +
  geom_label(aes(x = Zn, y = V, colour = Prov, label = ID))

# Create a scatter plot with labels showing the relationship between 'Fe2O3' and 'Al2O3', colored by 'Prov'
all_dis %>%
  rownames_to_column("ID") %>%
  ggplot() +
  geom_label(aes(x = Fe2O3, y = Al2O3, colour = Prov, label = ID))

# Calculate and display the final model accuracy
accuracy_final <- mean(predictions$class == test.transformed$Prov)
print(accuracy_final)

```


##visualization

```{r}
# Load necessary libraries for creating the table and scaling colors
library(gt)
library(scales)

# Convert the means from the LDA model to a dataframe
means <- as.data.frame(model$means)

# Get the column names from the means (which represent the elements/variables)
elements_mean <- colnames(means)

# Create a table using the gt package and apply a color gradient to each column
means %>%
  rownames_to_column("Site") %>%  # Add row names as a new column called "Site"
  gt() %>%  # Create the gt table
  data_color(  # Apply color gradient to numeric columns
    columns = elements_mean,  # Apply to all numeric columns
    colors = scales::col_numeric(  # Use col_numeric to apply a color scale
      palette = c("red", "yellow", "green"),  # Define the color palette (from red to green)
      domain = range(as.matrix(means[elements_mean]), na.rm = TRUE)  # Set the range of values across all columns
    )
  )

# Save the means dataframe to a CSV file
write.csv2(means, here("results/mean.csv"))

# Display the means data frame
as.data.frame(model$means)

```

## reference groups mean & sd

```{r}
# Create the reference_groups dataset by selecting columns: Region and any low.sd.elements.cl_10 columns
reference_groups <- RFA4 |> 
  dplyr::select(Region, any_of(low.sd.elements.cl_10)) |> 
  rename(Prov = Region)  # Rename "Region" to "Prov"

# Calculate standard deviation (relative to mean) for each element per group (Prov)
rg_sd <- reference_groups |> 
  group_by(Prov) |>  # Group by the "Prov" column
  summarise(across(everything(), ~ (sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE)) * 100))  # Relative standard deviation

# Calculate the mean for each element per group (Prov)
rg_mean <- reference_groups |> 
  group_by(Prov) |>  # Group by "Prov"
  summarise(across(everything(), ~ (mean(.x, na.rm = TRUE))))  # Calculate mean for each group

# Use gt() to create a table of means and apply a color gradient for each column
rg_mean %>%
  gt() %>%
  { 
    tbl <- .  # Store the initial table
    for (col in elements_mean) {  # Loop through each element in 'elements_mean' to apply color
      tbl <- tbl %>%
        data_color(  # Apply the color gradient
          columns = all_of(col),  # Specify which column to color
          colors = scales::col_numeric(  # Apply a numeric color scale
            palette = c("red", "yellow", "green"),  # Color palette from red to green
            domain = range(rg_mean[[col]], na.rm = TRUE)  # Define the color range from min to max of that column
          )
        )
    }
    tbl  # Return the table with color applied
  }

# Create another table for standard deviations (rg_sd) and apply a color gradient
rg_sd %>%
  rownames_to_column("Site") |>  # Convert row names to a column named "Site"
  gt() %>%  # Create a table
  data_color(  # Apply the color gradient to all columns
    columns = elements_mean,  # Apply to all columns in 'elements_mean'
    colors = scales::col_numeric(  # Use a numeric color scale
      palette = c("green", "yellow", "red"),  # Color palette from green to red
      domain = range(as.matrix(rg_sd[elements_mean]), na.rm = TRUE)  # Color range for all numeric columns
    )
  )

# Save the calculated mean and standard deviation tables to CSV files
write.csv2(rg_mean, here("results/rg_means.csv"))  # Save means table
write.csv2(rg_sd, here("results/rg_sd.csv"))  # Save standard deviations table
```


# reference groups pca
```{r}

# Filter the RFA.ref dataset:
# Remove rows where the Object column matches any value in outliers_all
loc <- RFA.ref |> 
  filter(!(Object %in% outliers_all)) |>  # Remove outliers
  column_to_rownames("Object")  # Set the "Object" column as row names

# Perform PCA on the filtered dataset "loc"
# The PCA analysis is based on the selected elements defined in 'low.sd.elements.cl_10'
pca_loc <- loc |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the specified elements for PCA
  PCA()  # Perform PCA analysis



```

```{r}
# Farbenblind-freundliche "Okabe-Ito"-Palette
# This defines a color palette for visualizations, designed to be colorblind-friendly
cb_palette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
                "#D55E00", "#CC79A7", "#999999", "#660066", "#FFB6C1")

# Definiere verschiedene Symbole für Gruppen (bis zu 10 Gruppen)
# This vector defines different shapes for points, up to 10 groups (symbols in ggplot2)
shape_symbols <- c(1, 17, 18, 15,  3, 7, 10, 13, 8, 11)

# Stelle sicher, dass loc$Region ein Faktor ist
# Ensures that the "Region" column in the 'loc' dataset is a factor variable (for grouping)
loc$Region <- as.factor(loc$Region)

# Visualisierung mit fviz_pca_ind
# This is a PCA plot using the first two principal components
pca_ref_ind_1_2 <- fviz_pca_ind(pca_loc,
             geom.ind = "point",        # Only show points (no lines or other markers)
             col.ind = loc$Region,      # Color points based on the "Region" factor
             palette = cb_palette,      # Apply the colorblind-friendly palette
          #   pointshape = unname(symbol_mapping[loc$Region]),  # Uncomment if you want different shapes per group
          #   addEllipses = TRUE,        # Uncomment to add ellipses around groups
           #  ellipse.type = "t",        # Uncomment to add t-distribution ellipses
            # ellipse.level = 0.75,      # Uncomment to set the confidence level of ellipses (default is 0.95)
             legend.title = "Groups"    # Title for the legend
)

# Visualisierung mit fviz_pca_ind
# Another PCA plot, but this time using the first and third principal components
pca_ref_ind_1_3 <- fviz_pca_ind(pca_loc,
                          axes = c(1,3),  # Set the axes to the first and third principal components
             geom.ind = "point",        # Only show points (no lines or other markers)
             col.ind = loc$Region,      # Color points based on the "Region" factor
             palette = cb_palette,      # Apply the colorblind-friendly palette
          #   pointshape = unname(symbol_mapping[loc$Region]),  # Uncomment if you want different shapes per group
            # addEllipses = TRUE,        # Uncomment to add ellipses around groups
           #  ellipse.type = "t",        # Uncomment to add t-distribution ellipses
           #  ellipse.level = 0.75,      # Uncomment to set the confidence level of ellipses (default is 0.95)
             legend.title = "Groups"    # Title for the legend
)

# PCA plot for variable contributions (cos2 values), showing the first two principal components
pca_ref_var_1_2 <- fviz_pca_var(pca_loc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), # Color gradient for variable cos2 values
             repel = TRUE # Avoid text overlap for variable labels
             )

# PCA plot for variable contributions (cos2 values), showing the third and fourth principal components
pca_ref_var_1_3 <- fviz_pca_var(pca_loc, col.var = "cos2",
             axes = c(3,4), # Set the axes to the third and fourth principal components
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), # Color gradient for variable cos2 values
             repel = TRUE # Avoid text overlap for variable labels
             )
```


```{r}
# Visualize PCA results for the first and second principal components
pca_ref_ind_1_2

# Visualize PCA results for the first and third principal components
pca_ref_ind_1_3

# Save the PCA plot for the first and second principal components to a .jpeg file
ggsave(here("results/plots/pca_ref_ind_1_2.jpeg"), plot = pca_ref_ind_1_2, device = "jpeg", dpi = 300, width = 8, height = 6)

# Save the PCA plot for the first and third principal components to a .jpeg file
ggsave(here("results/plots/pca_ref_ind_1_3.jpeg"), plot = pca_ref_ind_1_3, device = "jpeg", dpi = 300, width = 8, height = 6)

# Combine the PCA variable contribution plots (first & second, and third & fourth principal components)
# The plot_grid function arranges the two PCA variable contribution plots in a single row (ncol = 2)
pca_ref_var <- plot_grid(pca_ref_var_1_2, pca_ref_var_1_3, ncol = 2)

# Display the combined PCA variable contribution plots
pca_ref_var

# Save the combined PCA variable contribution plots to a .jpeg file
ggsave(here("results/plots/pca_ref_var.jpeg"), plot = pca_ref_var, device = "jpeg", dpi = 300, width = 8, height = 6)

```


# sgraffito
## mar sgraffito and all reference groups
```{r}
# Define a vector of objects related to 'Sgraffito' locations
sgrf_mar <- c("MAR3_208", "MAR2_39", "MAR1_4", "MAR3_209", "MAR3_28")

# Filter the data and process it
loc <- RFA.fin |> 
  # Filter the data to exclude outliers, but include specific objects (sgrf_mar)
  filter(!(Object %in% outliers_all) | Object %in% sgrf_mar) |> 
  # Further filter to exclude "Marandet" site, unless it's a specific object (sgrf_mar)
  filter(Site != "Marandet" | Object %in% sgrf_mar) |> 
  # Create a new column 'Prov' that assigns "Sgraffito" if Region contains "marandet" (case-insensitive), else "local"
  mutate(Prov = ifelse(grepl("marandet", Region, ignore.case = TRUE), "Sgraffito", "local")) |> 
  # Create a new column 'Region2' with custom region names for specific conditions
  mutate(Region2 = case_when(
    str_detect(Region, "Lake_Chad") ~ "Chad Basin",  # If "Lake_Chad" is in Region, rename as "Chad Basin"
    Region == "Marandet" ~ "sgraffito_marandet",  # If Region is "Marandet", rename to "sgraffito_marandet"
    TRUE ~ Region  # Leave all other values as is
  )) |> 
  # Create a new column 'Region3' with custom conditions, including renaming and categorization
  mutate(Region3 = case_when(
    str_detect(Region, "Lake_Chad") ~ Region,  # Keep "Lake_Chad" region as is
    Region == "Marandet" ~ "sgraffito_marandet",  # Rename "Marandet" region
    TRUE ~ "other"  # All other regions will be labeled as "other"
  )) |> 
  # Set the "Object" column as row names
  column_to_rownames("Object")

# Perform PCA on the selected data (low.sd.elements.cl_10) from the filtered and transformed dataset
pca_loc <- loc |> 
  dplyr::select(any_of(low.sd.elements.cl_10)) |>  # Select the columns from 'low.sd.elements.cl_10'
  PCA()  # Perform Principal Component Analysis

# Get the variable contributions to the first five principal components (for PCA)
pca_loc$var$contrib[, 1:5]  # Extract the contribution of each variable to t

```

```{r}

# Visualize PCA with points (Axes 1 and 2)
fviz_pca_ind(pca_loc,
             axes = c(1, 2),  # Show the first and second principal components
             geom.ind = "point",  # Display points only (no text)
             col.ind = loc$Region,  # Color points by 'Region' from the 'loc' dataset
             pointshape = 20,  # Shape of the points (20 represents a filled circle)
             cb_palette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
                             "#D55E00", "#CC79A7", "#999999", "#660066", "#FFB6C1"),  # Color palette (Okabe-Ito palette)
             addEllipses = TRUE,  # Add ellipses to the plot (to visualize groupings)
             ellipse.type = "t",  # Use t-distribution for ellipses
             ellipse.level = 0.75,  # Confidence level for ellipses (0.75 gives narrower ellipses)
             legend.title = "Groups"  # Legend title for the groups
)

# Visualize PCA with points (Axes 1 and 3)
fviz_pca_ind(pca_loc,
             axes = c(1, 3),  # Show the first and third principal components
             geom.ind = "point",  # Display points only (no text)
             col.ind = loc$Region,  # Color points by 'Region' from the 'loc' dataset
             pointshape = 20,  # Shape of the points (20 represents a filled circle)
             cb_palette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
                             "#D55E00", "#CC79A7", "#999999", "#660066", "#FFB6C1"),  # Color palette (Okabe-Ito palette)
             addEllipses = TRUE,  # Add ellipses to the plot (to visualize groupings)
             ellipse.type = "t",  # Use t-distribution for ellipses
             ellipse.level = 0.75,  # Confidence level for ellipses (0.75 gives narrower ellipses)
             legend.title = "Groups"  # Legend title for the groups
)

# Visualize variable contributions for the first and second principal components
fviz_pca_var(pca_loc, 
             col.var = "cos2",  # Color variables based on their squared cosine contribution
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Gradient color scale for variables
             repel = TRUE  # Avoid text overlap (to make the labels clearer)
)

# Visualize variable contributions for the first and third principal components
fviz_pca_var(pca_loc, 
             col.var = "cos2",  # Color variables based on their squared cosine contribution
             axes = c(1, 3),  # Show the first and third principal components
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # Gradient color scale for variables
             repel = TRUE  # Avoid text overlap (to make the labels clearer)
)

```
## Region2
```{r}
library(factoextra)
library(ggplot2)

# Colorblind-friendly "Okabe-Ito" palette
cb_palette <- c("#56B4E9", "#009E73", "#F0E442", "#999999","#0072B2", 
                "#D55E00", "#CC79A7",  "#660066", "#FFB6C1")

# Define different symbols for groups (up to 10 groups)
shape_symbols <- c(17, 18, 15, 8, 3, 7, 10, 13, 11)

# Ensure that loc$Region2 is a factor
loc$Region2 <- as.factor(loc$Region2)

# Create a mapping of Region -> Symbol
#region_levels <- levels(loc$Region2)
#symbol_mapping <- setNames(shape_symbols[seq_along(region_levels)], region_levels)

# Visualization with fviz_pca_ind (PCA scatter plot for axes 1 and 2)
pca_sgrf_ref_ind_1_2_g <- fviz_pca_ind(pca_loc,
             geom.ind = "point",        # Show points only (not "text")
             col.ind = loc$Region2,     # Color by Region2
             palette = cb_palette,      # Colorblind-friendly palette
             #   pointshape = unname(symbol_mapping[loc$Region2]),  # Use different symbols for each group
             #   addEllipses = TRUE,        # Add ellipses
             #   ellipse.type = "t",        # t-distribution for ellipses
             #   ellipse.level = 0.75,      # Confidence level (0.75 = tighter ellipse)
             legend.title = "Groups"
)

# Visualization with fviz_pca_ind (PCA scatter plot for axes 1 and 3)
pca_sgrf_ref_ind_1_3_g <- fviz_pca_ind(pca_loc,
                          axes = c(1,3),
             geom.ind = "point",        # Show points only
             col.ind = loc$Region2,     # Color by Region2
             palette = cb_palette,      # Colorblind-friendly palette
             #   pointshape = unname(symbol_mapping[loc$Region2]),  # Use different symbols for each group
             #   addEllipses = TRUE,        # Add ellipses
             #   ellipse.type = "t",        # t-distribution for ellipses
             #   ellipse.level = 0.75,      # Confidence level (0.75 = tighter ellipse)
             legend.title = "Groups"
)

# PCA variable contribution plot (axes 1 and 2)
pca_sgrf_ref_1_2 <- fviz_pca_var(pca_loc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

# PCA variable contribution plot (axes 1 and 3)
pca_sgrf_ref_1_3 <- fviz_pca_var(pca_loc, col.var = "cos2",
             axes = c(1,3),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

```


```{r}
# Save the PCA scatter plots for axes 1 and 2
ggsave(here("results/plots/pca_sgrf_ref_ind_1_2_g.jpeg"), plot = pca_sgrf_ref_ind_1_2_g, device = "jpeg", dpi = 300, width = 8, height = 6)

# Save the PCA scatter plots for axes 1 and 3
ggsave(here("results/plots/pca_sgrf_ref_ind_1_3_g.jpeg"), plot = pca_sgrf_ref_ind_1_3_g, device = "jpeg", dpi = 300, width = 8, height = 6)

# Load the cowplot package to combine multiple plots
library(cowplot)

# Combine the PCA variable contribution plots for axes 1 and 2, and axes 1 and 3
pca_sgrf_ref_var <- plot_grid(pca_sgrf_ref_1_2, pca_sgrf_ref_1_3, ncol = 2)

# Display the combined variable contribution plots
pca_sgrf_ref_var

# Save the combined PCA variable contribution plot
ggsave(here("results/plots/pca_sgrf_ref_var.jpeg"), plot = pca_sgrf_ref_var, device = "jpeg", dpi = 300, width = 8, height = 6)

```

## Region 3
```{r}
# Load required libraries
library(factoextra)
library(ggplot2)

# Define the color palette for the plot (colorblind-friendly "Okabe-Ito" palette)
cb_palette <- c("#E69F00", "#009E73", "#F0E442", "#0072B2", 
                "#D55E00", "#CC79A7", "#660066", "#999999", "#FFB6C1")

# Define various symbols for groups (up to 10 groups)
shape_symbols <- c(17, 18, 15, 3, 7, 10, 13, 8, 11)

# Ensure that 'loc$Region3' is treated as a factor variable for coloring
loc$Region3 <- as.factor(loc$Region3)

# Visualize PCA individuals (first two principal components: axes 1 and 2)
pca_sgrf_ref_ind_1_2_c <- fviz_pca_ind(pca_loc,
             geom.ind = "point",        # Show only points (no text)
             col.ind = loc$Region3,      # Color points by region
             palette = cb_palette,      # Apply colorblind-friendly palette
             legend.title = "Groups"    # Title for the legend
)

# Visualize PCA individuals (axes 1 and 3)
pca_sgrf_ref_ind_1_3_c <- fviz_pca_ind(pca_loc,
                          axes = c(1, 3),            # Use axes 1 and 3 for the plot
             geom.ind = "point",        # Show only points (no text)
             col.ind = loc$Region3,      # Color points by region
             palette = cb_palette,      # Apply colorblind-friendly palette
             legend.title = "Groups"    # Title for the legend
)

# Visualize PCA variables (showing contributions for axes 1 and 2)
fviz_pca_var(pca_loc, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE               # Avoid overlapping text
)

# Visualize PCA variables (showing contributions for axes 1 and 3)
fviz_pca_var(pca_loc, col.var = "cos2",
             axes = c(1, 3),           # Use axes 1 and 3 for the plot
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE               # Avoid overlapping text
)

```


```{r}
# Plot the PCA result for the first two principal components (1 and 2) and display
pca_sgrf_ref_ind_1_2_c

# Plot the PCA result for the first and third principal components (1 and 3) and display
pca_sgrf_ref_ind_1_3_c

# Save the first PCA plot (1 and 2 axes) as a .jpeg file with 300 dpi and 8x6 inch size
ggsave(here("results/plots/pca_sgrf_ref_ind_1_2_c.jpeg"), plot = pca_sgrf_ref_ind_1_2_c, device = "jpeg", dpi = 300, width = 8, height = 6)

# Save the second PCA plot (1 and 3 axes) as a .jpeg file with 300 dpi and 8x6 inch size
ggsave(here("results/plots/pca_sgrf_ref_ind_1_3_c.jpeg"), plot = pca_sgrf_ref_ind_1_3_c, device = "jpeg", dpi = 300, width = 8, height = 6)

```

# plotly

```{r}
# Function to generate a 3D PCA plot with customizable options
plot_3d_pca <- function(data, pca_res, color_by = "Region", shape_by = "Local", 
                        custom_colors = NULL, show_arrows = TRUE) {
  
  # Load necessary libraries
  library(plotly)
  library(FactoMineR)
  library(factoextra)
  library(dplyr)
  
  # Check if PCA results were provided, if not, perform PCA
  if (missing(pca_res)) {
    pca_res <- PCA(data, graph = FALSE)
  }

  # Extract eigenvalues (percentage of variance explained)
  eig_values <- get_eigenvalue(pca_res)

  # Convert PCA coordinates to a dataframe and add color group information
  pca_data <- as.data.frame(pca_res$ind$coord)
  pca_data[[color_by]] <- data[[color_by]]

  # Define point shapes if 'shape_by' is not FALSE, otherwise, set all points to circles
  if (is.logical(shape_by) && shape_by == FALSE) {
    shape_column <- rep("circle", nrow(pca_data))
  } else {
    pca_data[[shape_by]] <- as.factor(data[[shape_by]])  # Ensure 'shape_by' is a factor
    shape_column <- pca_data[[shape_by]]
  }

  # Extract loadings (arrows showing variable contributions to PCA)
  loadings <- as.data.frame(pca_res$var$coord[, 1:3])
  loadings$length <- sqrt(rowSums(loadings^2))  # Calculate length of each vector for scaling

  # Set default color palette if 'custom_colors' is not provided
  if (is.null(custom_colors)) {
    custom_colors <- c("#FFA500", "brown", "#00008B", "#00FFFF", "#6495ED", 
                       "#4682B4", "#ADD8E6", "#1E90FF", "#D2B48C")
  }
  
  # Define up to 10 different symbols for the shape of the points
  shape_symbols <- c("circle", "square", "diamond", "cross", "x", "triangle-up",
                     "triangle-down", "triangle-left", "triangle-right", "pentagon")

  # If there are more than 10 groups, repeat the symbols
  if (!is.logical(shape_by) || shape_by != FALSE) {
    unique_shapes <- length(unique(pca_data[[shape_by]]))
    if (unique_shapes > length(shape_symbols)) {
      shape_symbols <- rep(shape_symbols, length.out = unique_shapes)
    }
    shape_mapping <- setNames(shape_symbols[1:unique_shapes], unique(pca_data[[shape_by]]))
  } else {
    shape_mapping <- setNames(rep("circle", length(shape_column)), shape_column)
  }

  # Create 3D scatter plot with PCA individuals
  fig <- plot_ly(data = pca_data) %>%
    add_markers(x = ~Dim.1, 
                y = ~Dim.2, 
                z = ~Dim.3, 
                color = ~get(color_by),  # Color by the specified column
                symbol = shape_column,   # Shape based on the specified column
                colors = custom_colors,  # Apply the custom color palette
                symbols = shape_mapping) %>%
    layout(scene = list(xaxis = list(title = paste('Dim.1 (', round(eig_values[1, 2], 2), '%)', sep = '')),
                        yaxis = list(title = paste('Dim.2 (', round(eig_values[2, 2], 2), '%)', sep = '')),
                        zaxis = list(title = paste('Dim.3 (', round(eig_values[3, 2], 2), '%)', sep = ''))),
           title = "3D PCA Plot")

  # Add loading arrows (vectors) to the plot if 'show_arrows' is TRUE
  if (show_arrows) {
    colors <- colorRampPalette(c("blue", "red"))(nrow(loadings)) # Create a color gradient
    for (i in 1:nrow(loadings)) {
      fig <- fig %>% 
        add_trace(type = 'scatter3d', mode = 'lines',
                  x = c(0, loadings[i, 1]),
                  y = c(0, loadings[i, 2]),
                  z = c(0, loadings[i, 3]),
                  line = list(width = loadings$length[i] * 5, color = colors[i]), # Adjust width and color
                  name = rownames(loadings)[i],  # Add name to the legend
                  showlegend = TRUE) %>%
        add_text(x = loadings[i, 1], 
                 y = loadings[i, 2], 
                 z = loadings[i, 3], 
                 text = rownames(loadings)[i], 
                 textposition = 'top middle',
                 showlegend = FALSE)
    }
  }

  # Display the 3D plot
  return(fig)
}

# Call the function to plot the 3D PCA with the 'loc' dataset and the PCA results
plot_3d_pca(loc, pca_loc, shape_by = FALSE, custom_colors =  c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
                "#D55E00", "#CC79A7", "#999999", "#660066", "#FFB6C1"))

# Display the elements with low standard deviation (low.sd.elements.cl_10)
low.sd.elements.cl_10

```



# other statistics
## decor abundance
```{r}
# Create a bar plot from the 'pot_attr' data
pot_attr |> 

  ggplot(aes(x = JAS_decor, fill = JAS_decor)) +  # Map 'JAS_decor' to the x-axis and fill color
  geom_bar() +  # Create a basic bar chart
  ggtitle("Main decoration technique documented sherds (n=217)") +  # Add a title to the plot
  scale_fill_viridis_d(option = "viridis") +  # Use a color palette that is color-blind friendly (Viridis)
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate x-axis labels for better readability
  labs(x = "JAS Decor", y = "Anzahl", fill = "JAS Decor")  # Label the axes and the legend

```

## sherds measured and sherds in reference groups
```{r}
# Group data from the 'RFA.fin' dataset by 'Site' and 'Region' 
# and count the number of distinct 'Object' entries in each group
RFA.fin |> 
  group_by(Site, Region) |> 
  summarise(n = n_distinct(Object))

# Group data from the 'RFA.ref' dataset by 'Site' and 'Region' 
# and count the number of distinct 'Object' entries in each group
RFA.ref |> 
  group_by(Site, Region) |> 
  summarise(n = n_distinct(Object))

```
